---
title: "High Dimensionality Dataset Reduction Methodologies in Applied Machine Learning"
author_profile: false
toc: false

toc_sticky: true
tags:
  - mathjax
date: 2016-11-14 08:15:00 +0530
categories: courses
header:
  teaser: "/assets/images/ai1.jpg"
excerpt: "High Dimensionality Dataset Reduction Methodologies in Applied Machine Learning"
description: "High Dimensionality Dataset Reduction Methodologies in Applied Machine Learning"
og_image: "/assets/images/proj.jpg"
use_math: true
mathjax: true
---

<!--classes : wide-->

Farhan Hai Khan<sup>a</sup>, Tannistha Pal<sup>b</sup>

a. Department of Electrical Engineering, Institute of Engineering & Management, Kolkata, India, njrfarhandasilva10@gmail.com<br>
b. Department of Electronics and Communication Engineering, Institute of Engineering & Management, Kolkata, India, paltannistha@gmail.com

## **Abstract**

A common problem faced while handling multi-featured datasets is the high amount of dimensionality that they often consist of, leading to barriers in generalized hands-on Machine Learning. These datasets also give a drastic impact on the performance of Machine Learning algorithms, being memory inefficient and frequently leading to model overfitting. It often becomes difficult to visualize or gain insightful knowledge on the data features such as presence of outliers.

This chapter will help data analysts reduce data dimensionality using various methodologies such as:<br>

1. Feature Selection using Covariance Matrix
2. Principal Component Analysis (PCA)
3. t-distributed Stochastic Neighbour Embedding (t-SNE)

Under applications of Dimensionality Reduction Algorithms with Visualizations, firstly, we introduce the Boston Housing Dataset and use the Correlation Matrix to apply Feature Selection on the strongly positive correlated data and perform Simple Linear Regression over the new features.Then we use UCI Breast Cancer Dataset to perform PCA Analysis with Support Vector Machine Classification (SVM). Lastly, we apply t-SNE to MNIST Handwritten Digits Dataset and use k-Nearest Neighbours (kNNs) clustering for classification.

Finally, we explore the benefits of using Dimensionality Reduction Methods and provide a comprehensive overview of reduction in storage space, efficient models,feature selection guidelines ,redundant data removal and outlier analysis.

## **Keywords** : Dimensionality Reduction, Feature Selection, Covariance Matrix, PCA , t-SNE

## **Table of Contents**

1. Problems faced with Multi-Dimensional Datasets
   1. Data Intuition
   2. Data Visualization Constraints
   3. Outlier Detection
2. Dimensionality Reduction Algorithms with Visualizations
   1. Feature Selection using Covariance Matrix
   2. Principal Component Analysis (PCA)
   3. t-distributed Stochastic Neighbour Embedding (t-SNE)
3. Benefits of Dimensionality Reduction
   1. Storage Space Reduction
   2. Computation Time Optimization
   3. Redundant Feature Removal
   4. Incorrect Data Removal

## 1. Problems faced with High Dimensionality Data : An Introduction

<!--References
https://thenewstack.io/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning/

https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/#:~:text=Dimensionality%20reduction%20refers%20to%20techniques%20for%20reducing%20the%20number%20of,%E2%80%9Cessence%E2%80%9D%20of%20the%20data.

https://medium.com/@cxu24/why-dimensionality-reduction-is-important-dd60b5611543
-->

<!--
### 1.1 **_An Introduction on High Dimensional Data_**
-->

<!--
<i><blockquote> "...dimensionality reduction yields a more compact, more easily interpretable representation of the target concept, focusing the userâ€™s attention on the most relevant variables."</blockquote></i>

-Page 289, Data Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016.

https://amzn.to/2tlRP9V
-->

<!--
<p align='right'> Your Text </p>
<p style='text-align: right;'> Your Text </p>
<style>body {text-align: right}</style>
<div style="text-align: right"> your-text-here </div>
<p style="text-align:right">This is some text in a paragraph.</p>
-->

<blockquote> "Dimensionality Reduction leads to a comprehensive, precise & compressed depiction of the target output variables, by reducing redundant input variables." 
<p align='right'><b>- Farhan Khan & Tannistha Pal.</b></p>
</blockquote>

_In the field of artificial intelligence, data explosion has created a plethora of input data & features to be fed into machine learning algorithms. Since most of the real-world data is multi-dimensional in nature, data scientists & data analysts require the core concepts of dimensionality reduction mechanisms for better :_<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_(i) Data Intuition : Visualization, Outlier Detection & Noise Reduction_<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;_(ii) Performance Efficiency : Faster Training Intervals & Reduced Computational Processing Time_<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;_(iii) Generalization : Prevents Overfitting (High Variance & Low Bias)_<br/><br/>
_This chapter introduces the practical working implementation of these reduction algorithms in applied machine learning._

<!--
<p>Anything you'd like to mention goes here: <blockquote>"Insert actual quote here."</blockquote> - Mr. Name</p>
-->
<!--4 nbsp - non breaking space is a tab &nbsp;&nbsp;&nbsp;&nbsp;-->

<i>Multiple features make it difficult to obtain valuable insights into data, as the visualization plots obtained can be 3-Dimensional at most. Due to this limitation, dependent properties/operations such as Outlier Detection and Noise Removal become more and more non-intuitive to perform. Applying dimensionality reduction helps in identifying these properties more nonchalantly.</i>

<i>Due to this reduced/compressed form of data, faster mathematical operations such as Scaling, Categorization etc. can be performed. Also, the data is more cleansed and this further solves the issues of overfitting a model.</i>

<i>
Dimensionality Reduction can be broadly classified into :<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(i) Feature Selection Techniques : Feature Selection attempts to train the machine learning model by selectively choosing a subset of the original feature set based on some criteria. Hence, redundant and obsolete characteristics could be eliminated without much information loss. Examples - Correlation Matrix Thresholding & Chi Squared Test Selection.<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;(ii) Feature Extraction/Projection Techniques : This method projects the original input features from the high dimensional space by summarizing most statistics and removing redundant data / manipulating to create new relevant output features with reduced dimensionality (fewer dimensional space). Examples - Principle Component Analysis (PCA) , Linear Discriminant Analysis (LDA), t-distributed Stochastic Neighbour Embedding(t-SNE) & Isometric Mapping (IsoMap).<br/><br/></i>

_However, we have limited our discussion to Correlation Matrices, PCA & t-SNE only, as covering all such techniques is beyond the scope of this book/chapter._

<!--
TODO

3+ dims hard to visualize
outlier noise follow
 del unnecessary rows
Std scaler less time, lesss size,

Quick Tip
SEEDS
-->

## 2. Dimensionality Reduction Algorithms with Visualizations

### 2.1 **_Feature Selection using Covariance Matrix_**

**Objective :** Introduce Boston Housing Dataset and use the obtained Correlation Matrix to apply Feature Selection on the strongly positive correlated data and perform Regression over the selctive features.

<!-- References
https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155

https://www.geeksforgeeks.org/ml-boston-housing-kaggle-challenge-with-linear-regression/

https://towardsdatascience.com/polynomial-regression-bbe8b9d97491

Ridge & Lasso ??? -->

<!--Alert message.-->

We will need 3 datasets for this chapter, each of which have been documented on our github repository.
Hence we will create a local copy (clone) of that repo here.

<!--You can read more about git and github here.??-->

```python
!git clone https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION.git
```

    Cloning into 'DIMENSIONALITY_REDUCTION'...
    remote: Enumerating objects: 14, done.[K
    remote: Counting objects: 100% (14/14), done.[K
    remote: Compressing objects: 100% (12/12), done.[K
    remote: Total 14 (delta 1), reused 0 (delta 0), pack-reused 0[K
    Unpacking objects: 100% (14/14), done.

Firstly we will import all the necessary libraries that we will be requiring for Dataset Reductions.

```python
import numpy as np               # Mathematical Functions , Linear Algebra, Matrix Operations
import pandas as pd              # Data Manipulations,  Data Analysis/Storing/Preparation
import matplotlib.pyplot as plt  # Simple Data Visualization , Basic Plotting Utilities
plt.style.use("dark_background") #just a preference of the authors, adds visual attractiveness
import seaborn as sns            # Advanced Data Visualization, High Level Figures Interfacing
%matplotlib inline
# used for Jupyter Notebook Plotting
#%matplotlib notebook            # This can be used as an alternative as the plots obtained will be interactive in nature.
```

**Initial Pseudo Random Number Generator Seeds**

In Applied Machine Learning, it is essential to make experiments reproducable and at the same time keeping weights as completely random. The Seed of a Pseudo Random Number Generator (PRNG) acheives the exact same task by initializing values with the same conditions everytime a program is executed.

<!--Alert Box-->

For more info on PRNGs and Seeds visit falana.

<!--

Random Codes

import tensorflow as tf

# Set the seed for hash based operations in python
os.environ['PYTHONHASHSEED'] = '0'

# Set the numpy seed
np.random.seed(111)

# Disable multi-threading in tensorflow ops
session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

# Set the random seed in tensorflow at graph level
tf.set_random_seed(111)

# Define a tensorflow session with above session configs
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)

# Set the session in keras
K.set_session(sess)

# Make the augmentation sequence deterministic
aug.seed(111)
-->

```python
univ_seed=42
np.random.seed(univ_seed)
```

### **The Boston Housing Data :**

The Dataset is derived from information collected by the U.S. Census Service concerning housing in the area of Boston Mass.

Variables in order:

- CRIM - per capita crime rate by town
- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
- INDUS - proportion of non-retail business acres per town
- CHAS - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
- NOX - nitric oxides concentration (parts per 10 million)
- RM - average number of rooms per dwelling
- AGE - proportion of owner-occupied units built prior to 1940
- DIS - weighted distances to five Boston employment centres
- RAD - index of accessibility to radial highways
- TAX - full-value property-tax rate per \$10,000
- PTRATIO - pupil-teacher ratio by town
- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- LSTAT - % lower status of the population
- MEDV - Median value of owner-occupied homes in \$1000's

<!--References
 The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
 prices and the demand for clean air', J. Environ. Economics & Management,
 vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics
 ...', Wiley, 1980.   N.B. Various transformations are used in the table on
 pages 244-261 of the latter.
-->

Importing the dataset :

```python
from sklearn.datasets import load_boston
boston_dataset = load_boston()
df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
df['MEDV'] = boston_dataset.target
df.head(10)
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.02985</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.430</td>
      <td>58.7</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.12</td>
      <td>5.21</td>
      <td>28.7</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.08829</td>
      <td>12.5</td>
      <td>7.87</td>
      <td>0.0</td>
      <td>0.524</td>
      <td>6.012</td>
      <td>66.6</td>
      <td>5.5605</td>
      <td>5.0</td>
      <td>311.0</td>
      <td>15.2</td>
      <td>395.60</td>
      <td>12.43</td>
      <td>22.9</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.14455</td>
      <td>12.5</td>
      <td>7.87</td>
      <td>0.0</td>
      <td>0.524</td>
      <td>6.172</td>
      <td>96.1</td>
      <td>5.9505</td>
      <td>5.0</td>
      <td>311.0</td>
      <td>15.2</td>
      <td>396.90</td>
      <td>19.15</td>
      <td>27.1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.21124</td>
      <td>12.5</td>
      <td>7.87</td>
      <td>0.0</td>
      <td>0.524</td>
      <td>5.631</td>
      <td>100.0</td>
      <td>6.0821</td>
      <td>5.0</td>
      <td>311.0</td>
      <td>15.2</td>
      <td>386.63</td>
      <td>29.93</td>
      <td>16.5</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.17004</td>
      <td>12.5</td>
      <td>7.87</td>
      <td>0.0</td>
      <td>0.524</td>
      <td>6.004</td>
      <td>85.9</td>
      <td>6.5921</td>
      <td>5.0</td>
      <td>311.0</td>
      <td>15.2</td>
      <td>386.71</td>
      <td>17.10</td>
      <td>18.9</td>
    </tr>
  </tbody>
</table>
</div>

```
boston_dataset.keys()
```

    dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])

```
boston_dataset.feature_names
```

    array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',
           'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')

```
boston_dataset.DESCR
```

    ".. _boston_dataset:\n\nBoston house prices dataset\n---------------------------\n\n**Data Set Characteristics:**  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n     \n.. topic:: References\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n"

```
boston_dataset.filename
```

    '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/boston_house_prices.csv'

Alternatively you can also run the following code, provided you have cloned our repo. link

```
df= pd.read_excel("DIMENSIONALITY_REDUCTION/DATASETS/BOSTON.xlsx")
df= pd.read_excel("DIMENSIONALITY_REDUCTION/data/Boston_Data.xlsx")
```

Pro Tip/ Data Insights : You might want to try <code>df.isnull().sum()</code> , <code>df.info()</code> , <code>df.describe()</code> to get the columnwise null values, dataframe information and row-wise description respectively. However , here the data provided is clean and free from such issues which would be needed to be processed/handled inspectionally.

For more color palettes visit : link.

```
df.hist(bins=30,figsize=(20,10),grid=False,color="crimson"); #cyan , magenta , crimson,  #column="MEDV",,color='lime'
plt.savefig("Boston Dataset Frequency Distribution of Numerical Data.png",dpi=600)
```

![png](DSDA_Dimensionality_Reduction_Algorithms_Final_v2_files/DSDA_Dimensionality_Reduction_Algorithms_Final_v2_19_0.png)

```
correlation_matrix = df.corr().round(2)
# annot = True to print the values inside the square
plt.figure(figsize=(20,10))
sns.heatmap(data=correlation_matrix,cmap="inferno", annot=True)
plt.savefig("Correlation_Data.png",dpi=600)
```

![png](DSDA_Dimensionality_Reduction_Algorithms_Final_v2_files/DSDA_Dimensionality_Reduction_Algorithms_Final_v2_20_0.png)

```
df.columns
```

    Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
           'PTRATIO', 'B', 'LSTAT', 'MEDV'],
          dtype='object')

```
X = df.drop(columns=['MEDV'])
y = df['MEDV']

print('Shape of X : {} , Shape of y : {}'.format(X.shape,y.shape))
```

    Shape of X : (506, 13) , Shape of y : (506,)

```
# Normalization of the Data
from sklearn.preprocessing import StandardScaler

scaler_X = StandardScaler()
X_norm= scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_norm= scaler_y.fit_transform(X)


```

```
"abc {} xyz {}".format(1,2)
```

    'abc 1 xyz 2'

```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)
print('Shape of X_train : {} , Shape of X_test : {}'.format(X_train.shape,X_test.shape))
print('Shape of y_train : {} , Shape of y_test : {}'.format(y_train.shape,X_test.shape))
```

    Shape of X_train : (354, 13) , Shape of X_test : (152, 13)
    Shape of y_train : (354,) , Shape of y_test : (152, 13)

```
from sklearn.linear_model import LinearRegression

lin_model = LinearRegression()
lin_model.fit(X_train, y_train)
```

    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)

```
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score

accLR=lin_model.score(X_test,y_test)
print(accLR)
```

    0.7112260057484874

```
lin_model.coef_ #theta 1-13
```

    array([-1.33470103e-01,  3.58089136e-02,  4.95226452e-02,  3.11983512e+00,
           -1.54170609e+01,  4.05719923e+00, -1.08208352e-02, -1.38599824e+00,
            2.42727340e-01, -8.70223437e-03, -9.10685208e-01,  1.17941159e-02,
           -5.47113313e-01])

```
lin_model.coef_.shape
```

    (13,)

```
lin_model.intercept_ #theta 0
```

    31.631084035691632

```
df.columns
```

    Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
           'PTRATIO', 'B', 'LSTAT', 'MEDV'],
          dtype='object')

```
# Coefficient & Intercept :
def merge_list_to_dict(test_keys,test_values):
  """Uses dictionary comprehension to create a dict from 2 lists"""
  merged_dict = {test_keys[i]: test_values[i] for i in range(len(test_keys))}
  return merged_dict

def get_params(model_intercept,model_coefficients,data_cols):
  """Returns a dataframe of organised values for model parameters output with colums of the original dataframe"""
  res_theta=[model_intercept ]+ list(model_coefficients)
  res_y=['INTERCEPT']+list(data_cols)
  dict_res=merge_list_to_dict(res_y,res_theta)
  data=[dict_res]
  results=pd.DataFrame(data)
  return results

res=get_params(lin_model.intercept_,lin_model.coef_,X.columns)
res.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>INTERCEPT</th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>31.631084</td>
      <td>-0.13347</td>
      <td>0.035809</td>
      <td>0.049523</td>
      <td>3.119835</td>
      <td>-15.417061</td>
      <td>4.057199</td>
      <td>-0.010821</td>
      <td>-1.385998</td>
      <td>0.242727</td>
      <td>-0.008702</td>
      <td>-0.910685</td>
      <td>0.011794</td>
      <td>-0.547113</td>
    </tr>
  </tbody>
</table>
</div>

```
help(get_params)
```

    Help on function get_params in module __main__:

    get_params(model_intercept, model_coefficients, data_cols)
        Returns a dataframe of organised values for model parameters output with colums of the original dataframe

```
#demonstrating the purpose of the docstring
def tanni():
  """Meow meow meow"""
  return 1
help(tanni)
```

    Help on function tanni in module __main__:

    tanni()
        Meow meow meow

```


```

```

```

```

```

```

```

```

```

```
preds=lin_model.predict(X_test)

preds.mean()
```

    21.33015845218848

```
y_test.mean()
```

    21.407894736842103

```
temp=[]
for a,b in zip(preds,y_test):
  temp.append(abs(a-b))
temp=np.array(temp)/len(temp)
1-temp.mean()
```

    0.9791926982140957

```

```

```

```

```
df.columns
```

    Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
           'PTRATIO', 'B', 'LSTAT', 'MEDV'],
          dtype='object')

```

```

```

```

```

```

```
round(2.7346374,2)
```

    2.73

```
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def getevaluation(model, X_subset,y_subset,subset_type="Train",round_scores=None):
  y_subset_predict = model.predict(X_subset)
  rmse = (np.sqrt(mean_squared_error(y_subset, y_subset_predict)))
  r2 = r2_score(y_subset, y_subset_predict)
  mae=mean_absolute_error(y_subset, y_subset_predict)
  if round_scores!=None:
    rmse=round(rmse,round_scores)
    r2=round(r2,round_scores)
    mae=round(mae,round_scores)

  print("Model Performance for {} subset :: RMSE: {} | R2 score: {} | MAE: {}".format(subset_type,rmse,r2,mae))

getevaluation(model=lin_model,X_subset=X_train,y_subset=y_train,subset_type="Train",round_scores=2)
getevaluation(model=lin_model,X_subset=X_test,y_subset=y_test,subset_type="Test ",round_scores=2)
```

    Model Performance for Train subset :: RMSE: 4.75 | R2 score: 0.74 | MAE: 3.36
    Model Performance for Test  subset :: RMSE: 4.64 | R2 score: 0.71 | MAE: 3.16

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# model evaluation for training set
y_train_predict = lin_model.predict(X_train)



rmse = (np.sqrt(mean_squared_error(y_train, y_train_predict)))
r2 = r2_score(y_train, y_train_predict)

print("The model performance for training set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))
print("\n")

# model evaluation for testing set
y_test_predict = lin_model.predict(X_test)
rmse = (np.sqrt(mean_squared_error(y_test, y_test_predict)))
r2 = r2_score(y_test, y_test_predict)

print("The model performance for testing set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))

```

    The model performance for training set
    --------------------------------------
    RMSE is 4.748208239685937
    R2 score is 0.7434997532004697


    The model performance for testing set
    --------------------------------------
    RMSE is 4.638689926172867
    R2 score is 0.7112260057484874

## Metrics Report :

table

# ML Model with reduced dataset

The correlation coefficient ranges from -1 to 1. If the value is close to 1, it means that there is a strong positive correlation between the two variables. When it is close to -1, the variables have a strong negative correlation.

## Observations:

To fit a linear regression model, we select those features which have a high correlation with our target variable MEDV. By looking at the correlation matrix we can see that RM has a strong positive correlation with MEDV (0.7) where as LSTAT has a high negative correlation with MEDV(-0.74).
An important point in selecting features for a linear regression model is to check for multi-co-linearity. The features RAD, TAX have a correlation of 0.91. These feature pairs are strongly correlated to each other. We should not select both these features together for training the model. Check this for an explanation. Same goes for the features DIS and AGE which have a correlation of -0.75.
Based on the above observations we will use RM and LSTAT as our features. Using a scatter plot letâ€™s see how these features vary with MEDV.

```
df.columns
```

    Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
           'PTRATIO', 'B', 'LSTAT', 'MEDV'],
          dtype='object')

```
correlation_matrix = df.corr().round(2)

correlation_matrix
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>CRIM</th>
      <td>1.00</td>
      <td>-0.20</td>
      <td>0.41</td>
      <td>-0.06</td>
      <td>0.42</td>
      <td>-0.22</td>
      <td>0.35</td>
      <td>-0.38</td>
      <td>0.63</td>
      <td>0.58</td>
      <td>0.29</td>
      <td>-0.39</td>
      <td>0.46</td>
      <td>-0.39</td>
    </tr>
    <tr>
      <th>ZN</th>
      <td>-0.20</td>
      <td>1.00</td>
      <td>-0.53</td>
      <td>-0.04</td>
      <td>-0.52</td>
      <td>0.31</td>
      <td>-0.57</td>
      <td>0.66</td>
      <td>-0.31</td>
      <td>-0.31</td>
      <td>-0.39</td>
      <td>0.18</td>
      <td>-0.41</td>
      <td>0.36</td>
    </tr>
    <tr>
      <th>INDUS</th>
      <td>0.41</td>
      <td>-0.53</td>
      <td>1.00</td>
      <td>0.06</td>
      <td>0.76</td>
      <td>-0.39</td>
      <td>0.64</td>
      <td>-0.71</td>
      <td>0.60</td>
      <td>0.72</td>
      <td>0.38</td>
      <td>-0.36</td>
      <td>0.60</td>
      <td>-0.48</td>
    </tr>
    <tr>
      <th>CHAS</th>
      <td>-0.06</td>
      <td>-0.04</td>
      <td>0.06</td>
      <td>1.00</td>
      <td>0.09</td>
      <td>0.09</td>
      <td>0.09</td>
      <td>-0.10</td>
      <td>-0.01</td>
      <td>-0.04</td>
      <td>-0.12</td>
      <td>0.05</td>
      <td>-0.05</td>
      <td>0.18</td>
    </tr>
    <tr>
      <th>NOX</th>
      <td>0.42</td>
      <td>-0.52</td>
      <td>0.76</td>
      <td>0.09</td>
      <td>1.00</td>
      <td>-0.30</td>
      <td>0.73</td>
      <td>-0.77</td>
      <td>0.61</td>
      <td>0.67</td>
      <td>0.19</td>
      <td>-0.38</td>
      <td>0.59</td>
      <td>-0.43</td>
    </tr>
    <tr>
      <th>RM</th>
      <td>-0.22</td>
      <td>0.31</td>
      <td>-0.39</td>
      <td>0.09</td>
      <td>-0.30</td>
      <td>1.00</td>
      <td>-0.24</td>
      <td>0.21</td>
      <td>-0.21</td>
      <td>-0.29</td>
      <td>-0.36</td>
      <td>0.13</td>
      <td>-0.61</td>
      <td>0.70</td>
    </tr>
    <tr>
      <th>AGE</th>
      <td>0.35</td>
      <td>-0.57</td>
      <td>0.64</td>
      <td>0.09</td>
      <td>0.73</td>
      <td>-0.24</td>
      <td>1.00</td>
      <td>-0.75</td>
      <td>0.46</td>
      <td>0.51</td>
      <td>0.26</td>
      <td>-0.27</td>
      <td>0.60</td>
      <td>-0.38</td>
    </tr>
    <tr>
      <th>DIS</th>
      <td>-0.38</td>
      <td>0.66</td>
      <td>-0.71</td>
      <td>-0.10</td>
      <td>-0.77</td>
      <td>0.21</td>
      <td>-0.75</td>
      <td>1.00</td>
      <td>-0.49</td>
      <td>-0.53</td>
      <td>-0.23</td>
      <td>0.29</td>
      <td>-0.50</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>RAD</th>
      <td>0.63</td>
      <td>-0.31</td>
      <td>0.60</td>
      <td>-0.01</td>
      <td>0.61</td>
      <td>-0.21</td>
      <td>0.46</td>
      <td>-0.49</td>
      <td>1.00</td>
      <td>0.91</td>
      <td>0.46</td>
      <td>-0.44</td>
      <td>0.49</td>
      <td>-0.38</td>
    </tr>
    <tr>
      <th>TAX</th>
      <td>0.58</td>
      <td>-0.31</td>
      <td>0.72</td>
      <td>-0.04</td>
      <td>0.67</td>
      <td>-0.29</td>
      <td>0.51</td>
      <td>-0.53</td>
      <td>0.91</td>
      <td>1.00</td>
      <td>0.46</td>
      <td>-0.44</td>
      <td>0.54</td>
      <td>-0.47</td>
    </tr>
    <tr>
      <th>PTRATIO</th>
      <td>0.29</td>
      <td>-0.39</td>
      <td>0.38</td>
      <td>-0.12</td>
      <td>0.19</td>
      <td>-0.36</td>
      <td>0.26</td>
      <td>-0.23</td>
      <td>0.46</td>
      <td>0.46</td>
      <td>1.00</td>
      <td>-0.18</td>
      <td>0.37</td>
      <td>-0.51</td>
    </tr>
    <tr>
      <th>B</th>
      <td>-0.39</td>
      <td>0.18</td>
      <td>-0.36</td>
      <td>0.05</td>
      <td>-0.38</td>
      <td>0.13</td>
      <td>-0.27</td>
      <td>0.29</td>
      <td>-0.44</td>
      <td>-0.44</td>
      <td>-0.18</td>
      <td>1.00</td>
      <td>-0.37</td>
      <td>0.33</td>
    </tr>
    <tr>
      <th>LSTAT</th>
      <td>0.46</td>
      <td>-0.41</td>
      <td>0.60</td>
      <td>-0.05</td>
      <td>0.59</td>
      <td>-0.61</td>
      <td>0.60</td>
      <td>-0.50</td>
      <td>0.49</td>
      <td>0.54</td>
      <td>0.37</td>
      <td>-0.37</td>
      <td>1.00</td>
      <td>-0.74</td>
    </tr>
    <tr>
      <th>MEDV</th>
      <td>-0.39</td>
      <td>0.36</td>
      <td>-0.48</td>
      <td>0.18</td>
      <td>-0.43</td>
      <td>0.70</td>
      <td>-0.38</td>
      <td>0.25</td>
      <td>-0.38</td>
      <td>-0.47</td>
      <td>-0.51</td>
      <td>0.33</td>
      <td>-0.74</td>
      <td>1.00</td>
    </tr>
  </tbody>
</table>
</div>

```

```

```
to_keep=[""]
```

```

```

```

```

```

```

```

```

```

```

```

```

```
# Lasso/Ridge Regression

```

```

```

```

```

```

```

```

```

```

```

```

```

```
sns.pairplot(df);
```

    <seaborn.axisgrid.PairGrid at 0x7f7d53b7c828>

![png](DSDA_Dimensionality_Reduction_Algorithms_Final_v2_files/DSDA_Dimensionality_Reduction_Algorithms_Final_v2_77_1.png)

```

```

```

```

```

```

In this story, we applied the concepts of linear regression on the Boston housing dataset. I would recommend to try out other datasets as well.
Here are a few places you can look for data
https://www.kaggle.com/datasets
https://toolbox.google.com/datasetsearch
https://archive.ics.uci.edu/ml/datasets.html

## References

1. abc
2. bhshsh
3. jhgde
